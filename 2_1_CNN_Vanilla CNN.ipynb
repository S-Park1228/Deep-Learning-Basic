{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CNN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.sequence = list()\n",
    "        # 'Conv2D()': API for CNN\n",
    "        # the first arg: the number of output channels\n",
    "        # API recognizes the number of input channels itself.\n",
    "        # (no need to insert the number of input channels manually)\n",
    "        # the second arg: the size of kernel\n",
    "        # 'padding': default -> 'valid' (no zero padding)\n",
    "        #            'same' -> zero padding\n",
    "        # 28(H) X 28(W) X 16(C)\n",
    "        self.sequence.append(Conv2D(16, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # 28(H) X 28(W) X 16(C)\n",
    "        self.sequence.append(Conv2D(16, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # the arg of MaxPool2D: the size of pooling\n",
    "        # 14(H) X 14(W) X 16(C)\n",
    "        self.sequence.append(MaxPool2D((2, 2)))\n",
    "        # 14(H) X 14(W) X 32(C)\n",
    "        self.sequence.append(Conv2D(32, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # 14(H) X 14(W) X 32(C)\n",
    "        self.sequence.append(Conv2D(32, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # 7(H) X 7(W) X 32(C)\n",
    "        self.sequence.append(MaxPool2D((2, 2)))\n",
    "        # 7(H) X 7(W) X 64(C)\n",
    "        self.sequence.append(Conv2D(64, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # 7(H) X 7(W) X 64(C)\n",
    "        self.sequence.append(Conv2D(64, (3, 3), padding = 'same',\n",
    "                                    activation = 'relu'))\n",
    "        # the size of flattened vector -> 3136 (7 X 7 X 64)\n",
    "        self.sequence.append(Flatten())\n",
    "        self.sequence.append(Dense(128, activation = 'relu'))\n",
    "        self.sequence.append(Dense(10, activation = 'softmax'))\n",
    "        \n",
    "    def call(self, x, training = False, mask = None):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Training Loop\n",
    "@tf.function\n",
    "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# Implementing Test Loop\n",
    "@tf.function\n",
    "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(images)\n",
    "    \n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# rescaling\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 # x_train and x_test\n",
    "                                                  # become float64.\n",
    "# '..., tf.newaxis': adding an axis (axis for channel)\n",
    "# N X 28 X 28 -> N X 28 X 28 X 1 (N: the sample size of MNIST)\n",
    "# change the types of x_train and x_test into float32\n",
    "x_train = x_train[..., tf.newaxis].astype(np.float32)\n",
    "x_test = x_test[..., tf.newaxis].astype(np.float32)\n",
    "# 'from_tensor_slices': enables users to construct a dataset for a network\n",
    "# when they insert numpy objects or tensor objects.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss Function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Optimizer (algorithm for optimization)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# evaluation metric\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.08112213760614395, Accuracy: 97.52583312988281, Test Loss: 0.03843991085886955, Test Accuracy: 98.7699966430664\n",
      "Epoch: 2, Loss: 0.06439409404993057, Accuracy: 98.01499938964844, Test Loss: 0.034841831773519516, Test Accuracy: 98.92500305175781\n",
      "Epoch: 3, Loss: 0.0543186254799366, Accuracy: 98.32500457763672, Test Loss: 0.03318794444203377, Test Accuracy: 98.9366683959961\n",
      "Epoch: 4, Loss: 0.04779471829533577, Accuracy: 98.52633666992188, Test Loss: 0.0336960069835186, Test Accuracy: 98.93500518798828\n",
      "Epoch: 5, Loss: 0.0426211841404438, Accuracy: 98.68582916259766, Test Loss: 0.0330728217959404, Test Accuracy: 98.96600341796875\n",
      "Epoch: 6, Loss: 0.03863511607050896, Accuracy: 98.80738067626953, Test Loss: 0.03452499955892563, Test Accuracy: 98.96333312988281\n",
      "Epoch: 7, Loss: 0.0357552170753479, Accuracy: 98.89500427246094, Test Loss: 0.0335228405892849, Test Accuracy: 99.00143432617188\n",
      "Epoch: 8, Loss: 0.03304910659790039, Accuracy: 98.977783203125, Test Loss: 0.035600580275058746, Test Accuracy: 98.97999572753906\n",
      "Epoch: 9, Loss: 0.030942542478442192, Accuracy: 99.04083251953125, Test Loss: 0.03792552649974823, Test Accuracy: 98.94444274902344\n",
      "Epoch: 10, Loss: 0.029068009927868843, Accuracy: 99.10045623779297, Test Loss: 0.039510272443294525, Test Accuracy: 98.94000244140625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(model, images, labels, loss_object, optimizer,\n",
    "                  train_loss, train_accuracy)\n",
    "        \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(model, test_images, test_labels, loss_object,\n",
    "                  test_loss, test_accuracy)\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1}, ' +\n",
    "         f'Loss: {train_loss.result()}, ' +\n",
    "         f'Accuracy: {train_accuracy.result() * 100}, ' +\n",
    "         f'Test Loss: {test_loss.result()}, ' +\n",
    "         f'Test Accuracy: {test_accuracy.result() * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
