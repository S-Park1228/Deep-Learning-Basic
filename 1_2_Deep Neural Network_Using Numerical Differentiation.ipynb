{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Using Numerical Gradients\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_errors(h, y):\n",
    "    return 1 / 2 * np.mean(np.square(h - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W), x) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_inputs, num_outputs, activation = sigmoid):\n",
    "        def init_var(i, o):\n",
    "            # initial values for W matrix and b vector\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "        \n",
    "        self.sequence = list()\n",
    "        # First Hidden Layer\n",
    "        W, b = init_var(num_inputs, num_neuron)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        # Output Layer\n",
    "        W, b = init_var(num_neuron, num_outputs)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_gradient(self, x, y, loss_func):\n",
    "        def get_new_sequence(layer_index, new_neuron):\n",
    "            new_sequence = list()\n",
    "            for i, layer in enumerate(self.sequence):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_neuron)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "        \n",
    "        def eval_sequence(x, sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss = loss_func(self(x), y) # 'self(x)' is identical to 'self.__init__(x)'.\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence):\n",
    "            # w_i: the indices of neurons in a layer\n",
    "            for w_i, w in enumerate(layer.W):\n",
    "                for w_j, ww in enumerate(w):\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    new_seq = get_new_sequence(layer_id, Neuron(W, layer.b, layer.a))\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                        \n",
    "                    numerical_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                    layer.dW[w_i][w_j] = numerical_grad\n",
    "                \n",
    "            for b_i, bb in enumerate(layer.b):\n",
    "                b = np.copy(layer.b)\n",
    "                b[b_i] = bb + epsilon\n",
    "                    \n",
    "                new_seq = get_new_sequence(layer_id, Neuron(layer.W, b, layer.a))\n",
    "                h = eval_sequence(x, new_seq)\n",
    "                    \n",
    "                numerical_grad = (loss_func(h, y) - loss) / epsilon\n",
    "                layer.db[b_i] = numerical_grad\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha = 0.01): # alpha: learning rate\n",
    "    loss = network.calc_gradient(x, y, loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Loss: 0.7508902553268118.\n",
      "Epoch 2, Test Loss: 0.7466796412877036.\n",
      "Epoch 3, Test Loss: 0.7424943554657073.\n",
      "Epoch 4, Test Loss: 0.7383352805834973.\n",
      "Epoch 5, Test Loss: 0.7342032641579066.\n",
      "Epoch 6, Test Loss: 0.7300991177932463.\n",
      "Epoch 7, Test Loss: 0.7260236165939724.\n",
      "Epoch 8, Test Loss: 0.7219774986946541.\n",
      "Epoch 9, Test Loss: 0.717961464905691.\n",
      "Epoch 10, Test Loss: 0.7139761784716311.\n",
      "Epoch 11, Test Loss: 0.7100222649396308.\n",
      "Epoch 12, Test Loss: 0.7061003121342779.\n",
      "Epoch 13, Test Loss: 0.7022108702345.\n",
      "Epoch 14, Test Loss: 0.6983544519484891.\n",
      "Epoch 15, Test Loss: 0.6945315327820956.\n",
      "Epoch 16, Test Loss: 0.6907425513947959.\n",
      "Epoch 17, Test Loss: 0.6869879100390994.\n",
      "Epoch 18, Test Loss: 0.6832679750773109.\n",
      "Epoch 19, Test Loss: 0.6795830775698748.\n",
      "Epoch 20, Test Loss: 0.675933513930373.\n",
      "Epoch 21, Test Loss: 0.6723195466410367.\n",
      "Epoch 22, Test Loss: 0.6687414050231084.\n",
      "Epoch 23, Test Loss: 0.6651992860564389.\n",
      "Epoch 24, Test Loss: 0.661693355242805.\n",
      "Epoch 25, Test Loss: 0.6582237475078025.\n",
      "Epoch 26, Test Loss: 0.6547905681349736.\n",
      "Epoch 27, Test Loss: 0.6513938937286482.\n",
      "Epoch 28, Test Loss: 0.6480337731989784.\n",
      "Epoch 29, Test Loss: 0.6447102287652935.\n",
      "Epoch 30, Test Loss: 0.6414232569732274.\n",
      "Epoch 31, Test Loss: 0.6381728297206706.\n",
      "Epoch 32, Test Loss: 0.6349588952895697.\n",
      "Epoch 33, Test Loss: 0.6317813793785582.\n",
      "Epoch 34, Test Loss: 0.6286401861338538.\n",
      "Epoch 35, Test Loss: 0.625535199174668.\n",
      "Epoch 36, Test Loss: 0.6224662826100834.\n",
      "Epoch 37, Test Loss: 0.6194332820450754.\n",
      "Epoch 38, Test Loss: 0.6164360255721322.\n",
      "Epoch 39, Test Loss: 0.6134743247476911.\n",
      "Epoch 40, Test Loss: 0.6105479755498333.\n",
      "Epoch 41, Test Loss: 0.6076567593164448.\n",
      "Epoch 42, Test Loss: 0.6048004436618851.\n",
      "Epoch 43, Test Loss: 0.6019787833706025.\n",
      "Epoch 44, Test Loss: 0.5991915212670819.\n",
      "Epoch 45, Test Loss: 0.596438389060177.\n",
      "Epoch 46, Test Loss: 0.5937191081624255.\n",
      "Epoch 47, Test Loss: 0.5910333904820368.\n",
      "Epoch 48, Test Loss: 0.5883809391885174.\n",
      "Epoch 49, Test Loss: 0.585761449450583.\n",
      "Epoch 50, Test Loss: 0.5831746091467708.\n",
      "Epoch 51, Test Loss: 0.5806200995483821.\n",
      "Epoch 52, Test Loss: 0.5780975959746976.\n",
      "Epoch 53, Test Loss: 0.5756067684210843.\n",
      "Epoch 54, Test Loss: 0.5731472821593298.\n",
      "Epoch 55, Test Loss: 0.5707187983116919.\n",
      "Epoch 56, Test Loss: 0.5683209743977287.\n",
      "Epoch 57, Test Loss: 0.5659534648555445.\n",
      "Epoch 58, Test Loss: 0.5636159215371688.\n",
      "Epoch 59, Test Loss: 0.5613079941789046.\n",
      "Epoch 60, Test Loss: 0.5590293308474465.\n",
      "Epoch 61, Test Loss: 0.5567795783617314.\n",
      "Epoch 62, Test Loss: 0.5545583826921344.\n",
      "Epoch 63, Test Loss: 0.5523653893366053.\n",
      "Epoch 64, Test Loss: 0.5502002436756327.\n",
      "Epoch 65, Test Loss: 0.5480625913056879.\n",
      "Epoch 66, Test Loss: 0.5459520783526515.\n",
      "Epoch 67, Test Loss: 0.5438683517653754.\n",
      "Epoch 68, Test Loss: 0.5418110595905599.\n",
      "Epoch 69, Test Loss: 0.5397798512292814.\n",
      "Epoch 70, Test Loss: 0.5377743776764192.\n",
      "Epoch 71, Test Loss: 0.5357942917432147.\n",
      "Epoch 72, Test Loss: 0.5338392482638372.\n",
      "Epoch 73, Test Loss: 0.5319089042866629.\n",
      "Epoch 74, Test Loss: 0.5300029192512092.\n",
      "Epoch 75, Test Loss: 0.5281209551507873.\n",
      "Epoch 76, Test Loss: 0.526262676682145.\n",
      "Epoch 77, Test Loss: 0.5244277513826359.\n",
      "Epoch 78, Test Loss: 0.5226158497549471.\n",
      "Epoch 79, Test Loss: 0.5208266453809054.\n",
      "Epoch 80, Test Loss: 0.5190598150243521.\n",
      "Epoch 81, Test Loss: 0.5173150387236465.\n",
      "Epoch 82, Test Loss: 0.5155919998747224.\n",
      "Epoch 83, Test Loss: 0.5138903853049129.\n",
      "Epoch 84, Test Loss: 0.5122098853380623.\n",
      "Epoch 85, Test Loss: 0.5105501938517008.\n",
      "Epoch 86, Test Loss: 0.508911008326323.\n",
      "Epoch 87, Test Loss: 0.5072920298876191.\n",
      "Epoch 88, Test Loss: 0.5056929633418524.\n",
      "Epoch 89, Test Loss: 0.5041135172049681.\n",
      "Epoch 90, Test Loss: 0.5025534037256545.\n",
      "Epoch 91, Test Loss: 0.5010123389026409.\n",
      "Epoch 92, Test Loss: 0.4994900424971777.\n",
      "Epoch 93, Test Loss: 0.49798623804032827.\n",
      "Epoch 94, Test Loss: 0.4965006528357402.\n",
      "Epoch 95, Test Loss: 0.49503301795820853.\n",
      "Epoch 96, Test Loss: 0.49358306824851145.\n",
      "Epoch 97, Test Loss: 0.4921505423040591.\n",
      "Epoch 98, Test Loss: 0.4907351824666466.\n",
      "Epoch 99, Test Loss: 0.48933673480684303.\n",
      "Epoch 100, Test Loss: 0.48795494910557374.\n",
      "34.41136860847473 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth = 5, num_neuron = 32, num_inputs = 10, num_outputs = 2, activation = sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_errors, 0.01)\n",
    "    print(f'Epoch {epoch + 1}, Test Loss: {loss}.')\n",
    "print(f'{time.time() - t} seconds elapsed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
