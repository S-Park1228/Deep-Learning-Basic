{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`\n",
    "\n",
    "\n",
    "- One of the most popular framework for deep learning as well as TensorFlow (especially among deep learning researchers)\n",
    "\n",
    "\n",
    "- At first, it was called 'Torch' which was created based on 'Lua' language. Later, 'PyTorch' was released based on Python.\n",
    "\n",
    "\n",
    "- It was created by NYU and Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(9) # the same as NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(torch.arange(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tensor created by torch into numpy\n",
    "torch.arange(9).numpy() # the same as TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(9).reshape((3, 3)) # the same as NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a tensor from numpy array\n",
    "data = [[1, 2], [3, 4]]\n",
    "np_array = np.array(data)\n",
    "x_np_1 = torch.tensor(np_array) # Create a copy.\n",
    "                                # So, in some cases,\n",
    "                                # it might be waste of memory.\n",
    "print(x_np_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np_2 = torch.as_tensor(np_array) # Unlike 'torch.tensor',\n",
    "                                   # it creates a view.\n",
    "print(x_np_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np_3 = torch.from_numpy(np_array) # Unlike 'torch.tensor',\n",
    "                                    # it creates a view.\n",
    "print(x_np_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between 'x_np_1' and 'x_np_2' & 'x_np_3'\n",
    "x_np_1[0, 0] = 5\n",
    "print(x_np_1)\n",
    "print(np_array)\n",
    "\n",
    "x_np_2[0, 0] = 6\n",
    "print(x_np_2)\n",
    "print(np_array)\n",
    "\n",
    "x_np_3[0, 0] = 7\n",
    "print(x_np_3)\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a tensor from list\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3)) # the same as NumPy\n",
    "b = torch.zeros((2, 3)) # the same as NumPy\n",
    "c = torch.full((2, 3), 2) # cf) tf.fill, np.full\n",
    "d = torch.empty(2, 3)\n",
    "print(a)\n",
    "print(b) \n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.zeros_like(c) # the same as NumPy\n",
    "f = torch.ones_like(c) # the same as NumPy\n",
    "g = torch.full_like(c, 3)\n",
    "h = torch.empty_like(c) \n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.eye(3)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = torch.rand(2, 2)\n",
    "k = torch.randn(2, 2)\n",
    "print(j)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing attributes\n",
    "tensor = torch.rand(3, 4)\n",
    "tensor = tensor.reshape(4, 3)\n",
    "tensor = tensor.int() # default: 32 bits\n",
    "                      # for 16-bit integers,\n",
    "                      # tensor.int16()\n",
    "                      # or tensor.short()\n",
    "                      # See https://pytorch.org/docs/stable/tensors.html.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    \n",
    "print(f'Shape of tensor: {tensor.shape}')\n",
    "print(f'Datatype of tensor: {tensor.dtype}')\n",
    "print(f'Device tensor is stored on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([4, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Tensor Attributes\n",
    "t1 = torch.zeros(4, 3)\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "print(t1.dtype)\n",
    "print(t1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5940, 0.0221])\n",
      "torch.Size([2])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.rand(2)\n",
    "print(t2)\n",
    "print(t2.shape)\n",
    "print(t2.dtype)\n",
    "print(t2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype = torch.float32)\n",
    "y = torch.tensor([[5, 6], [7, 8]], dtype = torch.float32)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "==============================\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "==============================\n",
      "tensor([[ 2.7183,  7.3891],\n",
      "        [20.0855, 54.5981]])\n",
      "==============================\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print('=' * 30)\n",
    "print(torch.add(x, y))\n",
    "print(torch.subtract(x, y))\n",
    "print(torch.multiply(x, y)) # equal to x * y operation\n",
    "print(torch.divide(x, y))\n",
    "print('=' * 30)\n",
    "print(torch.exp(x))\n",
    "print('=' * 30)\n",
    "\n",
    "# All are elementwise except for the below.\n",
    "print(x @ y)\n",
    "print(torch.matmul(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-place operations (operations followed by '_')\n",
    "print(x.add(y))\n",
    "print(x)\n",
    "print(x.add_(y)) # in-place operation\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.arange(1, 11).reshape(2, 5)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = torch.sum(z, axis = 0) # in TensorFlow, tf.reduce_sum(z, axis = 0)\n",
    "sum2 = torch.sum(z, axis = 1)\n",
    "sum3 = torch.sum(z, axis = -1)\n",
    "print(sum1, sum1.shape)\n",
    "print(sum2, sum2.shape)\n",
    "print(sum3, sum3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating and stacking\n",
    "a = torch.arange(24).reshape(4, 6)\n",
    "b = a.clone().detach() # clone().detach(): a kind of methods copying tensors\n",
    "print(a, a.shape)\n",
    "print(b, b.shape)\n",
    "\n",
    "# concatenating\n",
    "c = torch.cat([a, b], axis = 0)\n",
    "print(c, c.shape)\n",
    "\n",
    "d = torch.cat([a, b], axis = -1)\n",
    "print(d, d.shape)\n",
    "\n",
    "# stacking - adding an axis\n",
    "e = torch.stack([a, b], axis = 0)\n",
    "print(e, e.shape)\n",
    "\n",
    "f = torch.stack([a, b], axis = -1)\n",
    "print(f, f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = torch.arange(9)\n",
    "nums * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(nums, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add(nums, 10).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For broadcasting semantics, see below. <br>\n",
    "https://pytorch.org/docs/stable/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing but applying 'reshape'\n",
    "range_nums = torch.arange(9).reshape(3, 3)\n",
    "range_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_nums.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_nums.view(1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< unsqueeze >\n",
      "t2: \n",
      "tensor([0.8278, 0.7893])\n",
      "torch.Size([2])\n",
      "\n",
      "---after unsqueeze---\n",
      "tensor([[0.8278, 0.7893]])\n",
      "torch.Size([1, 2])\n",
      "\n",
      "< reshape & permute >\n",
      "torch.Size([192])\n",
      "\n",
      "---after reshape---\n",
      "torch.Size([3, 8, 8])\n",
      "\n",
      "---after permute---\n",
      "torch.Size([8, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze: Increase a tensor's rank.\n",
    "# squeeze: Reduce a tensor's rank.\n",
    "# permute: dimension swap (e.g. (3, 1, 4) -> (4, 1, 3))\n",
    "t2 = torch.rand(2)\n",
    "print(\"< unsqueeze >\")\n",
    "print(\"t2: \")\n",
    "print(t2)\n",
    "print(t2.shape)\n",
    "\n",
    "print(\"\\n---after unsqueeze---\")\n",
    "t2_add_rank = t2.unsqueeze(0)\n",
    "print(t2_add_rank)\n",
    "print(t2_add_rank.shape)\n",
    "\n",
    "t4 = torch.rand(192) # 3 * 8 * 8\n",
    "print(\"\\n< reshape & permute >\")\n",
    "print(t4.shape)\n",
    "t4 = t4.reshape(3, 8, 8)\n",
    "print(\"\\n---after reshape---\")\n",
    "print(t4.shape)\n",
    "\n",
    "t4 = t4.permute(1, 2, 0) # arguments 0, 1, 2: indices in the original tensor\n",
    "print(\"\\n---after permute---\")\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2611, 0.6056, 0.1236, 0.2507, 0.4299, 0.3229, 0.5533, 0.2829, 0.8464,\n",
      "        0.1846, 0.3607, 0.2133])\n",
      "\n",
      "---after reshape---\n",
      "tensor([[[0.2611, 0.6056],\n",
      "         [0.1236, 0.2507]],\n",
      "\n",
      "        [[0.4299, 0.3229],\n",
      "         [0.5533, 0.2829]],\n",
      "\n",
      "        [[0.8464, 0.1846],\n",
      "         [0.3607, 0.2133]]])\n"
     ]
    }
   ],
   "source": [
    "t_5 = torch.rand(12)\n",
    "print(t_5)\n",
    "t_5 = t_5.reshape(3, 2, 2)\n",
    "print(\"\\n---after reshape---\")\n",
    "print(t_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(16).reshape(2, 2, 4)\n",
    "print(a, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.transpose(1, 2) # the same as swapaxes method in Numpy\n",
    "print(b, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.permute((2, 0, 1)) # the same as transpose in TensorFlow\n",
    "print(c, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_torch = torch.from_numpy(arr) # cf) tf.constant(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_torch.float() # tf.cast(tensor, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling device\n",
    "# Unlike TensorFlow, we need to set device.\n",
    "# Enable users to have access to GPU once the GPU is detected.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_torch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward() # backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "print(x.grad)\n",
    "\n",
    "# dy/dx = [[1, 1], [1,1]]\n",
    "# dz/dy = [[6y, 6y,], [6y, 6y]]\n",
    "# dout/dz = [[6y / n, 6y / n], [6y / n, 6y / n]] (n = 4 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch at a glance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn` vs. `torch.nn.functional`\n",
    "<br>\n",
    "torch.nn.functional: Only includes layers without parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "```\n",
    "- a python class defining a `Dataset`\n",
    "- called by `Dataset` implicitly\n",
    "- We need to implement the following methods of `Dataset`.\n",
    "```python\n",
    "__init__ # Initializes dir/file to read images.\n",
    "__getitem__ # Loads data from dataset at a given index.\n",
    "__len__ # Returns the size of dataset.\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "```\n",
    "- Deliver the dataset defined by the above to our model.\n",
    "- 3 parameters\n",
    "    - batch_size (default = 1)\n",
    "    - shuffle\n",
    "    - num_workers\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Example\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, csv_file = None, root_dir = None):\n",
    "        # self.csv_file = csv_file\n",
    "        # self.root_dir = root_dir\n",
    "        self.x_data = torch.rand(100, 4)\n",
    "        self.y_data = torch.rand(100, 1)\n",
    "        \n",
    "    # the size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "dataset = SimpleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size = 25, shuffle = True)\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# to divide the dataset into batches with the same length..\n",
    "# Add zero-paddings to the inputs with different length from those of others.\n",
    "def collate_fn(self, data):\n",
    "    max_len = 10\n",
    "    batch = []\n",
    "    for x, y in data:\n",
    "        x_padded = torch.cat([x, torch.zeros(max_len - x.shape[0])])\n",
    "        batch.append(x_padded)\n",
    "    return torch.stack(batch)\n",
    "\n",
    "dataset = SimpleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size = 25, collate_fn = dataset.collate_fn, shuffle = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINIST DATA\n",
    "\n",
    "# Download the training dataset from open dataset.\n",
    "training_data = datasets.MNIST(\n",
    "    root = 'data', # in the current working directory,\n",
    "                   # -> data -> MNIST\n",
    "    train = True, # training data or test data\n",
    "    download = True,\n",
    "    transform = Compose([ToTensor() # ,\n",
    "    # normalization\n",
    "    # Normalize(mean = (0.5, ), std = (0.5, ))                        \n",
    "                        ]))\n",
    "\n",
    "# Download the test dataset from open dataset.\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = Compose([ToTensor()]))\n",
    "\n",
    "# Or you do not need to use 'Compose()'.\n",
    "# test_data = datasets.MNIST(\n",
    "#     root = 'data',\n",
    "#     train = False,\n",
    "#     download = True,\n",
    "#     transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Generate dataloader.\n",
    "# DataLoader: enable a dataset to be iterable\n",
    "train_dataloader = DataLoader(training_data,\n",
    "                              batch_size = batch_size,\n",
    "                              shuffle = True)\n",
    "test_dataloader = DataLoader(test_data,\n",
    "                             batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print('Shape of X [N, C, H, W]:', X.shape) # in TensorFlow,\n",
    "                                               # N, H, W, C\n",
    "    print('Shape of y:', y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_image = torch.squeeze(images[0])\n",
    "torch_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch_image.numpy()\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "label = labels[0].numpy()\n",
    "print(label.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(label)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Or you can do the same thing this way.\n",
    "plt.imshow(images[0, 0, :, :], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device for training.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {device}.')\n",
    "\n",
    "# Define the model.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # (trainable) layers with weigths\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flattened = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    # layers without weights (fixed operation)\n",
    "    # called automatically by __call__\n",
    "    def forward(self, x):\n",
    "        x = self.flattened(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # model.train # train mode (active or deactive dropouts, batch-norm...)\n",
    "        \n",
    "        # prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # for every new batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'Loss: {loss:7f} [{current:>5d} / {size:>5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # evaludation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, ' +\n",
    "          f'Avg Loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t + 1}\\n------------------------------')\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# setting an image file's path\n",
    "cur_dir = os.getcwd()\n",
    "img_path = os.path.join(cur_dir, 'image.png')\n",
    "\n",
    "# opening the image file\n",
    "cur_img = Image.open(img_path)\n",
    "\n",
    "# resize it (28 X 28)\n",
    "cur_img = cur_img.resize((28, 28))\n",
    "image = np.asarray(cur_img)\n",
    "\n",
    "# convert it into RGB average (gray scale) if color image\n",
    "try:\n",
    "    image = np.mean(image, axis = 2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Since MNIST data have white letters and black backgrounds,\n",
    "# convert its letter and background into the same as those of the data.\n",
    "image = np.abs(255 - image)\n",
    "\n",
    "# rescaling\n",
    "image = image.astype(np.float32) / 255.0\n",
    "\n",
    "# printing\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.as_tensor(image).to(device).reshape(1, 1, 28, 28)\n",
    "model.eval()\n",
    "predict = model(image)\n",
    "print(f'The predicted value by the model is {predict.argmax(1).item()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides `Dataset` and `DataLoader` as a way to process a dataset and deliver it to a model\n",
    "\\\n",
    "\\\n",
    "Data and labels are stored in 'Dataset' and 'DataLoader' enables a dataset to be delivered to a model in terms of an iterable object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FasionMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = 'data', # directory in which a dataset is stored\n",
    "    train = True, # loading a training dataset\n",
    "    download = True,\n",
    "    transform = ToTensor() # converting data into PyTorch tensors\n",
    "                           # at the same time, rescaling applied (0 ~ 1)\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False, # loading a test dataset\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "labels_map = {\n",
    "    0: 'T-Shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot'\n",
    "}\n",
    "figure = plt.figure(figsize = (8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size = (1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating via DataLoader\n",
    "# displaying images and labels\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f'Feature batch shape: {train_features.size()}')\n",
    "print(f'Labels batch shape: {train_labels.size()}')\n",
    "img = train_features[0].squeeze() # 'squeeze()' removes\n",
    "                                  # dimensions which equal '1'\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show()\n",
    "print(f'Label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I need to use a customized dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example for customized Dataset/Transform/DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, np_data, transform = None):\n",
    "        self.data = np_data\n",
    "        self.transform = transform\n",
    "        self.len = np_data.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(sample):\n",
    "    return sample ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Compose([square])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.arange(10)\n",
    "custom_dataset = CustomDataset(np_data, transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataloader = DataLoader(custom_dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    for data in custom_dataloader:\n",
    "        print(data)\n",
    "    print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolusion Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple practice (modelling)\n",
    "nn.Conv2d(in_channels = 1, out_channels = 20, kernel_size = 5, stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can do the same thing this way.\n",
    "layer = nn.Conv2d(1, 20, 5, 1).to(torch.device('cpu'))\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "(20, 1, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weight visualization\n",
    "weight = layer.weight\n",
    "print(weight.shape)\n",
    "\n",
    "weight = weight.detach().numpy() # In order to convert weights into\n",
    "                                 # NumPy arrays, we need to apply\n",
    "                                 # detach() first.\n",
    "                                 # You will get an error if you try\n",
    "                                 # weight.numpy().\n",
    "print(weight.shape)\n",
    "\n",
    "plt.imshow(weight[0, 0, :, :], 'jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x2160 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output visualization\n",
    "output_data = layer(images[0:1]) # image: the variable which was generated\n",
    "                           # in the 'PyTorch at a glance' part\n",
    "output_data = output_data.data\n",
    "output = output_data.cpu().numpy()\n",
    "output.shape\n",
    "\n",
    "image_arr = images[0:1].numpy()\n",
    "print(image_arr.shape)\n",
    "\n",
    "plt.figure(figsize = (15, 30))\n",
    "plt.subplot(131)\n",
    "plt.title('Input')\n",
    "plt.imshow(np.squeeze(image_arr), 'gray')\n",
    "plt.subplot(132)\n",
    "plt.title('Weight')\n",
    "plt.imshow(weight[0, 0, :, :], 'jet')\n",
    "plt.subplot(133)\n",
    "plt.title('Output')\n",
    "plt.imshow(output[0, 0, :, :], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 14, 14])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.functional.max_pool2d(images[0:1], 2, 2)\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 14, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since MaxPool Layer does not have weights,\n",
    "# it can be directly applied 'numpy()'.\n",
    "pool_arr = pool.numpy()\n",
    "pool_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 15))\n",
    "plt.subplot(121)\n",
    "plt.title('Input')\n",
    "plt.imshow(np.squeeze(image_arr), 'gray')\n",
    "plt.subplot(122)\n",
    "plt.title('Output')\n",
    "plt.imshow(np.squeeze(pool_arr), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = images[0:1].view(1, 28 * 28) # Note that 'view' is nothing but\n",
    "                                      # 'reshape'.\n",
    "flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(784, 10)(flatten)\n",
    "lin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0099,  0.1138, -0.3225, -0.3118, -0.0420, -0.1401,  0.3334,  0.3875,\n",
       "         -0.3932, -0.2207]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(lin.detach().numpy(), 'jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0702, 0.1121, 0.1080, 0.0742, 0.1290, 0.1089, 0.0847, 0.0902, 0.1291,\n",
       "         0.0935]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to the results in terms of 'NumPy arrays',\n",
    "# we need to turn off weight.\n",
    "# That is, we need to invalidate the trainind mode flag.\n",
    "with torch.no_grad():\n",
    "    flatten = images[0:1].view(1, 28 * 28)\n",
    "    lin = nn.Linear(784, 10)(flatten)\n",
    "    softmax = nn.functional.softmax(lin, dim = 1)\n",
    "\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Class (the officially recommended approach by PyTorch)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flattened = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), # 'Dense' in TensorFlow\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10) # softmax is included in the cell\n",
    "                               # in which loss function is defined.\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flattened(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a model instance and setting device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a virtual dataset and prediction\n",
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted Class: {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'Loss: {loss:>7f} [{current:>5d} / {size:>5d}]')\n",
    "            \n",
    "# function for test\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad(): # for efficient memory management\n",
    "                          # no gradient calculations for a test dataset\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            # item(): Extract scalar from tensors.\n",
    "            # ex) If you want to get the number '1' in the tensor a = [[1]],\n",
    "            # do a.item().\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, ' +\n",
    "          f'Avg Loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t + 1}\\n------------------------------')\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a model and Loading the saved model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving only model's parameters\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model instance\n",
    "model2 = NeuralNetwork().to(device)\n",
    "print(model2)\n",
    "\n",
    "# test\n",
    "model2.eval()\n",
    "test_loop(test_dataloader, model2, loss_fn)\n",
    "\n",
    "# loading the saved parameters\n",
    "model2.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# test\n",
    "model2.eval()\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model itself\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "model3 = torch.load('model.pth')\n",
    "\n",
    "# test\n",
    "model3.eval()\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet18 = models.resnet18(pretrained = True)\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# If you want to reduce the number of neurons in the output layer,\n",
    "# follow the below codes.\n",
    "# Recall that the number of neurons in the ResNet output layer is 1000.\n",
    "num_classes = 10\n",
    "last_layer_in_features = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(last_layer_in_features, num_classes)\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./logs/pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new model instance and setting device\n",
    "model4 = NeuralNetwork().to(device)\n",
    "print(model4)\n",
    "\n",
    "model4.eval()\n",
    "test_loop(test_dataloader, model4, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device = device)\n",
    "writer.add_graph(model4, X) # X: for the model's sturcture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'Loss: {loss:7>f} [{current:d} / {size:5d}]')\n",
    "            \n",
    "        total_loss += loss / len(dataloader) # the number of batches\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, ' +\n",
    "         f'Avg Loss: {test_loss:>8f} \\n')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two layers in the model.\n",
    "parameters = ['Weight1', 'Bias1', 'Weight2', 'Bias2']\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t + 1}\\n------------------------------')\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    writer.add_scalar('training_loss', train_loss, t)\n",
    "    for param, name in zip(model.parameters(), parameters):\n",
    "        writer.add_histogram(name, param, t)\n",
    "    test_loss = test(test_dataloader, model, loss_fn)\n",
    "    writer.add_scalar('test_loss', test_loss, t)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close() # After writing, close the writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir '.logs/pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
